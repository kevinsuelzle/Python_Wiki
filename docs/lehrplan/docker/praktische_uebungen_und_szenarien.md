# Praktische √úbungen

**Projekt Arbeitszeiterfassung**

    Das Projekt besteht aus einem Pyton Programm, einem nginx Webserver und einer MS SQL Server Datenbank.
    
    Die Absicht ist es, √ºber Postman geeignete REST Anfragen an das Projekt zu senden.
    
    Im Anschluss daran k√∂nnen die √ºbertragenen Daten per REST Anfrage abgerufen werden.

### **Aufgabe: Projekt üå∂Ô∏èüå∂Ô∏èüå∂Ô∏è**

Projektbeschreibung:

Das Projekt

- soll ein JSON Objekt √ºber einen HTTP Request entgegennehmen
- die Daten in einer SQL Server Datenbank ablegen
- die vollst√§ndige Liste der vorhandenen Datens√§tze in Form von JSON Objekten zur√ºckgeben

Wir ben√∂tigen dazu:

- einen Empf√§nger, der die Anfragen annimmt. Dazu verwenden wir nginx.
- eine Software, die die Daten verarbeitet und an die Datenbank sendet. Dazu gibt es ein Python Backend Programm. Diese
  werden wir von GitHub herunterladen.
- eine Software, die die Datenbank abfragt und die Daten zur√ºckschickt. Das gleiche Programm.
- die Datenbank selbst. Wir verwenden SQL Server von Microsoft.
- eine Software, die es erm√∂glicht direkt mit der Datenbank zu arbeiten. Dies wird f√ºr Wartungsarbeiten am Server
  ben√∂tigt.
- eine Software, die HTTP Anfragen sendet und deren Antworten darstellt.

Dargestellt wird das mit Containern, die auf folgenden Images basieren:

- nginx:latest (der Webserver)
- python:3.9.18-slim (ein oft verwendetes stabiles linux) darauf basiert unser Python Programm
- mcr.microsoft.com/mssql/server:2019-latest (die Datenbank)

F√ºr die Kommunikation mit der datenbank gibt es eine ganze Reihe von Programmen:

- Azure Data Studio
- DBeaver
- Kommandozeilen Tool von Microsoft
- Visual Studio Code
- etc...

Nahezu jede Entwicklungsumgebung kann HTTP Anfragen senden und empfangen. Da wir mit PyCharm arbeiten ist hier der
Men√ºpunkt `Tools`->`HTTP-Client`->`Create Http Request` der Weg zum Ziel.

Konfiguration:

- nginx.conf

    ```nginx
    events {}
    
    http {
        server {
          listen 80;
    
          location / {
             proxy_pass http://localhost:3000;
             proxy_http_version 1.1;
             proxy_set_header Upgrade $http_upgrade;
             proxy_set_header Host $host;
             proxy_set_header content-type "application/json";
             proxy_cache_bypass $http_upgrade;
             proxy_set_header Connection 'upgrade';
         }
        }
    }
    ```

  **Anmerkung:**

  Wir gehen hier nicht auf alle Punkte der nginx.conf ein.

  Wichtig zu sehen ist,
    - dass dieser Webserver Anfragen auf die Basisadresse `/` (location)
    - am externen Port 80 (listen 80) entgegennimmt
    - und dann an den internen Port 3000 weiterleitet (proxy_pass).

- dockerfile

  Das Dockerfile f√§llt etwas komplexer aus, da die Konfiguration des Images auf die Verwendung des Microsoft SQL Servers
  eingestellt werden muss. Es m√ºssen Treiber geladen werden und in den Container eingebaut werden.
  ```dockerfile
  
  # Use an official Python runtime as a parent image
  # This is the latest official image based on debian linux
  # We ensure to use a amd64 image. That is needed to make the sql drivers work
  FROM --platform=linux/amd64 python:3.9.18-slim
  
  # Set the working directory in the container
  WORKDIR /app
  
  # Install dependencies required for pyodbc and the ODBC driver
  # apt is the debian package manger for installing programs 
  # At first we update all packages in the container to their latest version
  RUN apt-get update \
      # Then we install some programs that we need
      && apt-get install -y --no-install-recommends \
          # curl is a program to get stuff from a certain internet repository
          curl \
          # gnupg is a program to deal with certificates, a security measure
          gnupg \
          # the unix/linux version of odbc driver \
          # odbc means Open DataBase Connector \
          unixodbc \
          unixodbc-dev \
          # g++ is the GNU C++ compiler needed for several programs 
          g++ \
      # this curl command loads a security key from microsoft and add it to the key list
      && curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - \
      # this curl command loads some stuff from microsoft to enable the installation of the needed driver
      && curl https://packages.microsoft.com/config/debian/10/prod.list > /etc/apt/sources.list.d/mssql-release.list \
      # again update all packages to their latest versions  
      && apt-get update \
      # accept the rules of use for microsoft products and finally install the driver
      && ACCEPT_EULA=Y apt-get install -y --no-install-recommends msodbcsql18 \
      # Clean up
      && apt-get clean \
      && rm -rf /var/lib/apt/lists/*
  # now we copy the list of packages required for our python program into the container
  COPY ./requirements.txt /app
  # Install any needed packages specified in requirements.txt
  RUN pip install --no-cache-dir -r requirements.txt
  
  # Copy the current directory contents into the container at /usr/src/app
  # This now the program we have written
  COPY . .
  # An environment variable should be set to enable us command line commands for this app. 
  ENV FLASK_APP=App.py
  # Finally run app.py when the container launches
  CMD ["python", "./app.py"]
  ```
  **Fazit:**

  Wenn wir ein Python Programm auf unserem Computer erstellen, m√ºssen wir √ºber `√¨mport` Pakete einbinden, die uns
  notwendige Funktionen bereitstellen. Im Hintergrund laufen Prozess, die den Pythoncode zur Laufzeit des Programmes
  interpretieren und damit erst ausf√ºhrbar machen.

  All das wei√ü ein offizielles Basis-Image nicht.

  √úber das `RUN` Kommando im Dockerfile arbeiten wir alle Schritte ab, die notwendig sind, um das Python Programm
  lauff√§hig und ausf√ºhrbar zu machen. Es stellt unsere Umgebung so her, dass sie unserer lokalen Umgebung entspricht.

- docker-compose.yaml

  ```yaml
  version: '3'
  services:
    nginx:
      image: nginx:latest
      volumes:
        - ./nginx.conf:/etc/nginx/nginx.conf:ro
      ports:
        - "80:80"
  
    sql-server:
      image: mcr.microsoft.com/mssql/server:2019-latest
      environment:
        - ACCEPT_EULA=Y
        - SA_PASSWORD=Sql12345
      ports:
        - "1433:1433"
      volumes:
        - ./mydatabase:/var/opt/mssql
  
    node-app:
      build:
        context: .
        dockerfile: Dockerfile
      network_mode: service:nginx
      depends_on:
        - sql-server
  ```

  **Anmerkung:**

  Diese YAML-Datei definiert drei Services:

    - nginx: der Webserver mit dem Bind-Mount nginx.conf und dem Port Mapping 80:80.
      So kann die Konfigurationsdatei von der Festplatte gelesen werden und Anfragen an den Port 80 werden registriert
      und
      weiter verarbeitet wie in
      der Konfigurationsdatei beschrieben.
    - sql-server: der SQL Server von Microsoft wird in der aktuellen Version 2019 geladen, seine Lizenzbestimmungen
      werden
      akzeptiert und das Passwort f√ºr den allgemeinen Nutzer `sa`wird festgelegt. Der Port 1433, der Standardport von
      SQL-Server, wird intern verwendet und nach au√üen hin freigegeben. So k√∂nnen wir von anderen Anwendungen auf dem
      lokalen Rechner auf die Datenbank zugreifen.
    - node-app: ein Container, der nach Vorschriften des Dockerfile zu bauen ist und der auf den Webserver h√∂ren soll.
      Zudem
      ist er abh√§ngig von der Datenbank.

- Vorbereitung der Datenbank

  Jede Verbindung zu einer Datenbank braucht mindestens vier Verbindungsparameter:
    - host: der Computer, auf dem die Datenbank l√§uft. Hier `localhost`
    - port: der Port, auf dem die Datenbank auf dem Host erreichbar ist. Hier `1433`
    - user: der Benutzername. Hier `sa`
    - password: das Passwort des Benutzers. Hier `Sql12345`

  Hier ein Beispiel aus PyCharm Professional:

  ![img_8.png](img_8.png)

  Stellen wir also die Verbindung zur Datenbank her. Die Tools verwenden unter Umst√§nden eigene Treiber. Hier muss
  individuell geschaut und recherchiert werden.

  Sobald die Verbindung steht, brauchen wir eine Kommandozeile, um mit der Datenbank zu reden. Diese √∂ffnet sich oft
  schon automatisch und es wird auch ein Strukturbaum der Datenbank angezeigt. Falls nicht, suchen sie nach
  Einstellungen, die diesen Strukturbaum anzeigt.

  In unserer Anwendung wollen wir eine Verbindung zur Datenbank herstellen. Diese existiert aber noch nicht. Daher muss
  √ºber ein Tool unserer Wahl, die Datenbank mit dem Kommando `CREATE DATABASE Zeiterfassung;` erstellt werden.

  ![img_9.png](img_9.png)

  Geben sie das Kommando in die Konsole ein und f√ºhren sie es aus. Im Strukturbaum sollte jetzt die Datenbank zu sehen
  sein.

  Damit sind alle Vorbereitungen f√ºr die Datenbank abgeschlossen.

- Das Python-Programm

  Starten sie PyCharm und erstellen sie ein neues Projekt aus folgender √∂ffentlicher
  Quelle: `https://github.com/Joeatc/endpoint-app-python.git`.

  Da dieser Kurs sich auf Docker bezieht, werden wir nicht im Detail auf die Python Programmierung eingehen.

  Das Programm ist so nicht lauff√§hig. Hierzu w√§ren einige Schritte zu erledigen, die wir uns aus dem gleichen Grund
  sparen.

- Container erstellen und starten

  √ñffnen sie nun die Konsole in PyCharm und f√ºhren sie folgenden Befehl aus:

  ```bash
  docker compose up -d 
  ```
  Dies weist die Docker Engine an, die docker-compose.yml Datei abzuarbeiten. Sie sehen, wie die Images geladen werden
  und wie das IMage f√ºr das Python Programm gebaut wird. Viele Schritte laufen ab, bis das Image endlich fertig ist.
  Endlich werden die drei Container gestartet.

  Pr√ºfen sie in Docker Desktop den Status der Container, schauen sie in die Log Dateien hinein. Ist alles in Ordnung?

  ![img_3.png](img_3.png)

- Das Python-Programm

  Im Verzeichnis `/routes/` werden drei m√∂glich Routen f√ºr das Programm definiert:
    - `/` die Wurzel Route. Hier wird nur ein einfache Text ausgegeben.
    - `getAllWorkItems` die Route holt alle erfassten Arbeitszeiten aus der Datenbank und gibt sie als JSON Objekte
      zur√ºck.
    - `insertWorkItem` diese Route nimmt ein JSON Objekt mit den Daten einer Arbeitseinheit und sendet es an die
      Datenbank.

- Arbeiten mit dem Programm

  Um mit dem Programm arbeiten zu k√∂nnen, m√ºssen HTTP Anfragen gesendet werden.
  Hier die Version mit Postman Daten senden (POST request):

  ![img_4.png](img_4.png)

  Hier die Abfrage der Datenbank (GET request):

  ![img_5.png](img_5.png)

Wenn die Anfragen erfolgreich abgearbeitet werden, ist unsere Anwendung einsatzbereit.

Sie arbeiten alle im selben Netzwerk. Finden sie die Adresse ihres Computers √ºber
`Systemsteuereung->Netzwerk->WLAN-details`

![img_6.png](img_6.png)

Versuchen sie folgendes:

- Ver√§ndern sie die Daten im POST Request so, dass der Text im Feld `activity` sie eindeutig erkenntlich macht.
- Ver√§ndern sie auch Uhrzeiten oder Datum, beachten sie dabei die amerikanische Schreibweise.
- Geben sie mehrere Datens√§tze ein.
- Pr√ºfen sie mit dem GET Request die erfolgreiche √úbertragung und Speicherung ihrer Eingabe
- tauschen sie untereinander die IP-Adressen und ersetzen sie in den POST/GET Requests `localhost` durch die
  IP-Adresse: `http://192.168.178.42/insertWorkItem` zum Beispiel.

Sie sollten jetzt die Daten ihres Tauschpartners sehen oder neue Daten zu seiner Datenbank hinzuf√ºgen k√∂nnen.

## Zusammenfassung

Das Beispiel soll ihnen einen Eindruck verschaffen, welches Aufwandes es bedarf, um eine solch einfache Funktion
bereitzustellen.
Andererseits erkennen sie auch, das ein einfaches `docker compose up -d` durchaus in der Lage ist, eine hochkomplexe
Anwendung aus dem Internet zu laden, aufzubauen und zu starten.
W√ºrden wir unseren selbst gebautes Image auf Docker Hub ablegen, so w√ºrde der Bau des Images noch entfallen.

Nicht ber√ºcksichtigt in dieser Applikation wurden Sicherheitsaspekte, Datenschutz und Benutzerverifizierung.
Solche Arbeiten k√∂nnten weitere Container erledigen. Die App w√ºrde als zentrale Stelle daf√ºr infrage kommen, meist
werden jedoch einige Routen von nginx direkt an die passenden Container weitergeleitet (proxy_pass und location). Dies
h√§ngt ganz von der Anwendung selbst ab.  



